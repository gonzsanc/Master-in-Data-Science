{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MAP\n",
    "\n",
    "Un map es siempre una función uno a uno en el sentido de que 100 inputs producen 100 outputs. ej.\n",
    "\n",
    "A -> (edad 5, altura 100)\n",
    "B -> (edad 10, altura 200)\n",
    "...\n",
    "...\n",
    "\n",
    "Ejemplo de map hecho en python:\n",
    "{code}[p.gen,p.edad for p in persona]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Clave/Valor -> para pasarlo a la función REDUCE deben de estar los datos en este formato.\n",
    "F,33\n",
    "M,32\n",
    "M,28\n",
    "\n",
    "La función reduce siempre recibe las claves ordenadas.\n",
    "El hadoop el orden de las claves sí está garantizado, pero en python groupbyKey no lo está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f1910346310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = [\n",
    "    (\"Pepe\",\"M\",32)\n",
    "     ,(\"Veronica\",\"F\",37)\n",
    "     ,(\"Juan\",\"M\",28)\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pepe', 'M', 32), ('Veronica', 'F', 37), ('Juan', 'M', 28)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Emulamos una rdd que se hubiera leído de los datos pero que se ha puesto en la lista.\n",
    "rdd = sc.parallelize(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pepe', 'M', 32), ('Veronica', 'F', 37), ('Juan', 'M', 28)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Al darle a sc.parallelize no hace nada porque todavía no se le ha pedido que haga algo.\n",
    "#collect recupera todos los datos de la rdd de la lista  \n",
    "#Ahora:\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# rdd -> la r es de \"resilient\" porque puede reutilizar los datos para recálculo.\n",
    "# Se puede hacer también mediante caché, guardando el resultado de la rdd y accediendo posteriormente pues tiene buen\n",
    "#rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cálculo de la media de las edades sin spark.\n",
    "import math\n",
    "m  = [x[2] for x in ps]\n",
    "sum(m)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Al usar collect, si tenemos una lista enorme, la memoria no lo soporta.\n",
    "sum([x[2] for x in rdd.collect()])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.333333333333336"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La función map tiene otra función dentro de sí misma.\n",
    "rdd.map(lambda x: x[2]).mean()\n",
    "#La x de la función lambda será cada uno de los elementos de la rdd.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 32), ('F', 37), ('M', 28)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vamos a calcular la media por género.\n",
    "kv = rdd.map(lambda x: (x[1],x[2]))\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', <pyspark.resultiterable.ResultIterable at 0x7f19066d9850>),\n",
       " ('F', <pyspark.resultiterable.ResultIterable at 0x7f19066d99d0>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 37]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para calcular la media tenemos que crear una función a propósito.\n",
    "#El groupbykey es un reduce. Para ahora calcular la media tenemos que crear un map.\n",
    "#Queremos transformar 2 elementos devueltos por groupbykey en otros dos elementos que son las medias. \n",
    "#Por eso hemos de utilizar un map.\n",
    "\n",
    "kv.groupByKey().map(lambda x: sum(x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 30), ('F', 37)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para no tener que insertar la clave se puede usar mapValues:\n",
    "kv.groupByKey().mapValues(lambda x: sum(x)/len(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "couponPath = \"/home/dsc/Data/spark/data/coupon150720.csv\"\n",
    "transmPath = \"/home/dsc/Data/spark/data/transm150720.csv\"\n",
    "#tar jxvf data.tar.bz2 para descomprimirlo desde la shell."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Descripción de los datos:\n",
    "\n",
    "Al ir a volar, se hace una reserva, se paga y se genera un ticket por persona con el itinerario completo del viaje.\n",
    "En un ticket hay varios (contiene) cupones -> {cupon1, ..., cupon16}, aunque en la práctica suele haber un máximo de 4 cupones. Ejemplo:\n",
    "los cupones de la ida:\n",
    "Cupon1 -> MAD LHR\n",
    "Cupon2 -> LHR JFK\n",
    "cupones de una vuelta:\n",
    "Cupon 3 JFK FRA\n",
    "Cupon 4 FRA MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
