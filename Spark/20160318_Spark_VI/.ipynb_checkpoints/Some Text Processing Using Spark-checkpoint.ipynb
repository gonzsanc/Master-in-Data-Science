{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraphs = sc.newAPIHadoopFile('../shakespeare.txt', \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\"org.apache.hadoop.io.LongWritable\", \"org.apache.hadoop.io.Text\",     conf={\"textinputformat.record.delimiter\": '\\n\\n'}).map(lambda l:l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanParagraphs = paragraphs.map(lambda paragraph: re.sub('[^a-zA-Z0-9 ]','',paragraph.lower().strip()))\n",
    "cleanParagraphs = cleanParagraphs.map(lambda paragraph: re.sub('[ ]+',' ',paragraph))\n",
    "cleanParagraphs = cleanParagraphs.filter(lambda l: l!=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histog = cleanParagraphs.map(lambda e: Counter(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({u'0': 1, u'1': 1, u'6': 1, u'9': 1}),\n",
       " Counter({u' ': 1, u'e': 2, u'h': 1, u'n': 2, u'o': 1, u's': 2, u't': 2})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histog.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En vez de utilizar palabras va a hashearlas, esto es, representarlas por un número.\n",
    "HashingTF convierte cada palabra en un hash con un un máximo de 2^20\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flatmap -> hago un map y lo aplano, pasando de ser una lista de listas a un vector simple.\n",
    "es como el unlist de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WordInDoc = cleanParagraphs.flatMap(lambda p: p.split(\" \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInDoc = WordInDoc.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordInDoc = wordInDoc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29134"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordInDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inicializar proceso de clálculo\n",
    "hasingTF = HashingTF(wordInDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-TDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF - cálculo\n",
    "tf = hasingTF.transform(cleanParagraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(29134, {7489: 1.0, 8425: 1.0, 22210: 1.0, 23458: 1.0}),\n",
       " SparseVector(29134, {1188: 2.0, 2748: 1.0, 3372: 2.0, 4993: 1.0, 16225: 1.0, 17161: 2.0, 18097: 2.0})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No salen términos porque está haciendo un hashing, salen números en vez de palabras...\n",
    "tf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No funciona muy bien el hasingTF. Pero es bastante bueno."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La implementación produce colisiones -> 2 palabras con el mismo número.\n",
    "Los números a veces no los hashea bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasingTF.indexOf('year')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
